## logstash的解析规则识别与数据流生成

### 解析规则识别

```
 input ---push---> queue <---pull--- filter_func ---> output_func
```
每个filter_fuc包含一块由规则识别的ruby代码，其中包含若干分支条件和filter插件的实例，分支条件中包含在对应条件下执行的filter插件实例调用
每个output_fuc包含一块由规则识别的ruby代码，其中包含若干分支条件和output插件的实例，分支条件中包含在对应条件下执行的output插件实例调用

### 数据流生成
每个pipeline实例中会初始化一个消息队列，该消息队列由input线程组负责生产，filter-output线程组负责消费。filter-output消费的数据，首先经过filter再经过解析后会经由ouput的，当最终消费完成后会从消息队列中。该队列以下称为“queue"

首先会根据传入参数设置”queue“队列每次消费的的大小和延时等属性。并当运行标志处于运行态时进入循环。循环的第一部分就是从"queue"中获取数据以消费，数据保存在一个名为”batch“的变量中（一个logstash定义的类实例，用于对从消息队列中获取的数据进行管理）。数据batch首先会经过filter（对应代码"filter_batch(batch)"），再将batch中经过filter的数据设置为output可以接收的状态（对应代码"flush_filters_to_batch(batch)"）。output再从batch中获取对应的数据，通过每个output的"multi_receive"函数将event发送到各个输出端。"multi_receive"函数由每个插件进行重写。

### 数据处理
在源码中对接收事件的处理是通过队列将input与filter解耦，分别为队列的生产者与消费者，每个filter线程获取到event后，会分别调用在pipeline中记录的filter插件，但filter是否真的执行了，是由filter_func各个函数决定的，这里会根据在解析规则文件中的filter组件配置的规则决定，并生成相应ruby代码，如果filter外包含if等条件语句，那么在解析规则识别时为生成的filter_func也会有相应的条件判断去选择对应的filter插件执行。
ref：
https://github.com/elastic/logstash/blob/e9c9865f4066b54048f8d708612a72d25e2fe5d9/logstash-core/lib/logstash/config/config_ast.rb
https://github.com/elastic/logstash/blob/5c36bc02f8784c3cdd50e99f43fd9487f1bc0a8a/logstash-core/lib/logstash/pipeline.rb#L241

## vector的规则识别与数据流生成

### 规则识别
vector的数据流结构由source，transform和output三种组件组合得到。
数据流中的数据流动方向为 source -> transform -> output
vector的配置使用的时toml格式，每种组件在使用者配置数据流时，需要为每个组件声明一个名字。
每个组件通过inputs配置决定该与哪些上游组件组合。
下方的配置模板代表了一个数据流

```
                                                    +---> transforms.apache_sampler ---> sinks.es_cluster
sources.apache_logs ---> transforms.apache_parser---+
                                                    +---> sinks.s3_archives
```

```toml
# Set global options
data_dir = "/var/lib/vector"

# Ingest data by tailing one or more files
[sources.apache_logs]
  type         = "file"
  include      = ["/var/log/apache2/*.log"]    # supports globbing
  ignore_older = 86400                         # 1 day

# Structure and parse the data
[transforms.apache_parser]
  inputs       = ["apache_logs"]
  type         = "regex_parser"                # fast/powerful regex
  patterns      = ['^(?P<host>[w.]+) - (?P<user>[w]+) (?P<bytes_in>[d]+) [(?P<timestamp>.*)] "(?P<method>[w]+) (?P<path>.*)" (?P<status>[d]+) (?P<bytes_out>[d]+)$']

# Sample the data to save on cost
[transforms.apache_sampler]
  inputs       = ["apache_parser"]
  type         = "sampler"
  rate         = 50        

# Send structured data to a short-term storage
[sinks.es_cluster]
  inputs       = ["apache_sampler"]            # only take sampled data
  type         = "elasticsearch"
  host         = "http://79.12.221.222:9200"   # local or external host
  index        = "vector-%Y-%m-%d"             # daily indices

# Send structured data to a cost-effective long-term storage
[sinks.s3_archives]
  inputs       = ["apache_parser"]             # don't sample for S3
  type         = "aws_s3"
  region       = "us-east-1"
  bucket       = "my-log-archives"
  key_prefix   = "date=%Y-%m-%d"               # daily partitions, hive friendly format
  compression  = "gzip"                        # compress final objects
  encoding     = "ndjson"                      # new line delimited JSON
  [sinks.s3_archives.batch]
    max_size   = 10000000                      # 10mb uncompressed
```

### 数据流生成
1. 在组件拓扑结构调整阶段
    1. 首先对source组件的依赖进行调整，例如将以source为input的transform组件任务进行调整，使得transfrom组件从原source调整数据输入为新的source组件
    2. 对transform组件进行调整
    3. 对sink组件进行调整

2. 在数据流启动阶段
    1. 首先对source组件进行启动
    * 已有source如果需要变动则首先重建
    * 对已有source重建完成后创建新增加的source
    2. 其次对Transform组件进行启动处理
    3. 最后对sink进行处理
    注: 首先启动input，再启动transform和sink，这个意味这数据接收在数据处理之前进行，如果这个过程中没有对数据进行缓存，会导致数据丢失，在logstash早期版本也是这么设计，但是在最新版本中已经将input放到最后启动。而在vector中数据的输入输出通过rust的异步数据能力，结合组件的拓扑图解构依赖避免了这个问题

各个组件间的解耦是通过数据流拓扑结构确定其依赖，在数据流启动或重新加载阶段完成对数据流上下游依赖的处理。



## splunk的数据处理规则识别与数据处理

### 解析规则识别
splunk的数据流由三个配置文件描述的对象组成，分别为input，props和output
* 在output中描述的对象为事件输出对象，负责将数据处理后生成的event输出到索引服务器或其他splunk的事件接收器中
* 在input中描述的对象为数据接收对象，该对象决定如何接收数据，并决定将对应数据传递到哪个output对象中
    * input在数据接收阶段，还会为数据进行CHARSET字符集编码形成可读字符串
    * 同时input阶段还会根据配置或一些潜规则为数据形成source,sourcetype或host等数据处理阶段可用元信息
    * input中的各种ROUTING配置决定其输出，从而完成输入输出的数据流创建
* props中描述的对象为数据处理对象，props根据对象的规则，决定如何处理input接收到的数据，并进行分行，时间提取，事件生成与事件字段提取等数据处理能力

### props配置文件的规则识别

#### 数据处理对象如何选择数据进行处理
props中定义的每个语句块都会根据一个数据处理的规则，一下将一个语句块定义的数据处理称为一个数据处理对象。
数据处理对象通过判断语句块的头部匹配规则是否与数据的元信息匹配决定是否处理数据。数据处理对象匹配数据的规则有5种，分别是通过匹配数据的sourcetype,source,host,rulename和delayedrule(即props.conf中的5种语句块起始定义)。如果数据处理对象与数据元信息相匹配则对应数据将会被数据处理对象处理，为了便于描述，将语句块的起始行内容称为**语句块名称或数据处理对象名称**，名称中含有其匹配数据的规则
```
例如语句块如下，我们称其名称为"apache_error"，而他匹配的数据就是，sourcetype等于apache_error的数据
[apache_error]
SHOULD_LINEMERGE = True
```

### 语句块合并
在多个语句块定义时，会存在数据块的合并和覆盖情况。
1. 一种情况是，相同名称的多个语句块，在生成数据对象时，那么语句块中不同的配置项将会被合并，而相同配置项则会覆盖(这里为猜测，覆盖或冲突报错，未进行实验)，在合并后生成对应的数据处理对象。个人认为是静态的覆盖和合并，即在配置完成后，接入数据前程序即可明确将多个语句块合并为一个数据处理对象
2. 还有一种猜测的覆盖和合并行为，是动态的。即生成了多个数据处理对象，而在数据接收时，该数据能够满足多个数据处理对象，但是数据处理对象间存在优先级，从而从中选择了某个数据处理对象。

在splunk的官方文档中`source::<apache_error>`的优先级大于`[source]`，所以前者会"overwrite"后者，在官方文档中没有说明这种"overwrite"是如何进行的，是在创建数据处理对象时的静态覆盖还是在接收数据时的动态覆盖，但是个人认为不可能时静态的覆盖，因为两个如果没有数据接入完全没有进行覆盖的依据
如果props.conf内容如下
```
[apache_error]
SHOULD_LINEMERGE = True

[source::apache_error]
SHOULD_LINEMERGE = False
```
在创建数据处理对象时，两个语句块完全没有谁覆盖谁的依据，无法决定到底该丢弃谁。只有可能时在数据接收和处理时对两个数据处理对象进行抉择，从而完成"overwrite"。当一条数据只含有信息"sourcetype=apache_error"时，那么会被数据处理对象`[apache_error]`处理，而如果同时含有信息"sourcetype=apache_error,source=apache_error"时，通过优先级策略决定了，应该被数据处理对象`[source::apache_error]`处理
并且语句块名称支持正则表达式，这种语句块必然难以与其他语句块进行静态的合并。

其优先级的策略为 `[source::<source>]` > `[host::<host>]` > `[sourcetype]`,并且字符匹配的语句块优先级 >模式匹配的语句块优先级。同时支持对语句块的语句块的优先级设置可以处理`[<sourcetype>]`或`[host::<host>]`语句块间的冲突,决定谁覆盖谁。然而不能改变不同种类的\<spec>间的覆盖行为，例如无论`[<sourcetype>]`的优先级有多高，都不会改变`[host::<host>]`配置会覆盖`[<sourcetype>]`配置的行为

个人猜测，splunk在数据处理对象生成时，将拥有相同名称的多个数据块合并生成一个数据处理对象。名称不同的语句块生成多个数据处理对象，对不同的数据处理对象进行分类排序，形成一个数据处理对象列表。在数据进入系统时遍历列表选择满足数据的数据处理对象进行处理，在处理完成后不再被其他数据处理对象继续处理。

### 关联数据处理规则
在props中接收数据时主要涉及的数据处理能力有分行、时间戳识别和字段提取。在此过程中数据分行和时间戳识别都是在props.conf中定义，作用于原始的数据上。而字段提取是在数据分行，时间戳识别，事件生成后进行。字段提取可以在原始数据上也可以在指定字段中，并且对应的提取规则往往在额外的transform.conf配置文件中进行配置。

每个数据处理对象的字段提取，通过在语句块中添加`TRANSFORMS-<class> = <transform_stanza_name>, <transform_stanza_name2>,...`语句与transform.conf中定义的提取方法相关联，用多个逗号分割的transform名，并按顺序处理。也可以支持一个语句块中定义多个"TRANSFORMS"，多个"TRANSFORMS"间按位置顺序执行。

### 数据流生成与数据处理
在splunk中input和props都是基于数据相连接的。
input接收数据，并将数据进行初步的处理，添加额外的信息。

props通过语句块生成多个数据处理对象，而数据props处理模块后，对应处理模块通过数据处理对象的数据特征条件，决定处理input数据的数据对象。

props数据处理对象会首先对原始数据进行时间戳识别，数据分行，最终生成数据事件。最后props为数据对象处理调用相关联的transform字段提取规则。

而props相关处理完成后，由于在input配置中已经确定好了output，因此会根据其配置输出到对应的output中

```
         + ---> props0(transform0, trnsfrom1) ---> props4(transform3)
input ---+ ---> props1(transform2, trnsfrom3)
 |       + ---> props2(transform1, trnsfrom0)
 |       + ---> props3(transform3, trnsfrom2)
 |
 |
 +---> output


input: CHARSET(存疑) ---> sourcetype(host, source) ---> EVENTBREAKER_ENABLED(也许是多行事件的特别处理标志) 

props: CHARSET(checked，存疑)  ---> aggregation(分行与时间提取，事件生成，事件生成存疑) ---> typing(SEDCMD, TRANSFORMS) ---> indexing(SEGMENTATION)

inline(EXTRACT) ---> alias ---> field extraction(REPORTER) ---> EVAL(like parell) ---> LOOKUP

input ---> parsingQueue ---> parsing Pipeline ---> indexQueue ---> indexing Pipeline
```

## 几种数据流的比较
* logstash中的数据流数据流的好处在于，他对插件的执行条件相对灵活，可以让使用者基于数据内容进行一定程度的调整。但是坏处在于其filter_func或output_func本质上是所有数据处理代码的大乱炖，就缺失了代码的并行能力，而且将多种数据混杂在一块而没有区分，当数据出现脏数据或者数据处理规则存在冲突时，容易弄脏输出源，而且不利于排查和掌握数据流处理的内部情况，出现问题时难以排查。由于filter_func代码的混杂，使用者在添加的处理规则达到一定程度后难以确定其执行顺序和处理逻辑，也难以确定数据的执行逻辑是否正确。为了解决这个问题，logstash在后续提供了pipe的概念希望将多个不同的数据处理流程分开配置和识别

* vector的数据流每一个组件都通过`inputs`配置与上一个组件强关联，不存在组件执行条件这个概念，唯一的潜在条件就是上游组件有新数据生成。并且vector数据流在构建时就已经将每个组件的输入输出进行组装(代码写得真好)，即静态的保证了数据流的正确性。在组件处理数据后，只需要将其输出，而不需要在数据处理运行时额外的处理来确保数据流向的正确。并且在配置改变时，仅需要调整其中改变的组件，并通过调整组件对象的输入输出，最小代价进行reload。但是缺点也是类似的vector无法在运行时动态的决定其数据的下游输出，而在实际情况中，往往会存在一个input接收多种数据，随后根据接收数据的元信息确定其下游。并且同一数据处理方法无法复用，即使是相同的处理流程也需要添加额外的数据处理组件配置。(即无法处理input和transform的"一对多"关系)

* splunk的数据流处理中，在配置input时就已经界定了output，两者强关联，其优点和缺点类似于vector的。而在props中的数据处理，则提供了动态的匹配选择能力，每个input必然只会对应一个props中定义的数据处理对象，在被某个数据处理对象处理完成后，才会选择下一个匹配的数据处理对象。不会存在像logstash中存在的数据规则混淆，数据容易混杂问题。

而一个数据处理对象中定义了相关的分行，时间提取，字段提取规则。字段提取规则在进行语句块定义时可以清晰的确定其执行顺序。
由于支持对input数据在接收时动态选择数据处理对象的能力，也能够解决vector无法一个input对应多种数据的问题。在input接收的多种数据类型，同一经过一个数据处理对象后可以划分为多种数据类型，再次被匹配的数据处理对象进行处理。即使得input和数据处理对象能够形成"多对多"的关系
由于多个数据处理对象相互独立，因此可以并行执行提高效率。

缺点则是，splunk在提供动态匹配选择数据处理模块中支持了模式匹配，因此在选择处理模块时可能会需要额外的匹配时间(也许吧)，数据匹配处理逻辑也比vector和logstash更为复杂(如果已经被某个数据处理对象处理后，必须确保数据不会相同的数据处理对象被二次处理，否则会死循环)。另一个确定是，如果数据需要通过多个数据处理模块(例如多次类型转换)进行处理，那么其数据流在处理对象较多的情况数据流语义可能不会如vector清晰(对人而言)，但相比logstash还是好了很多。对于这个缺点，也许可以考虑像logstash一样引入pipeline的概念将，相关性较高的数据放在同一pipeline中，一来可以减少每次选择数据处理对象的数据，二来语义可能会清晰一些。

通过解析配置文件的生成数据流图的方式比较清晰的展示整个数据流图，但是如果在每个数据处理对象中对数据类型的转换使用了字段匹配从日志中提取字段，那么数据流图也无法清晰的展示数据的流转。例如一个input接收来自多个机器的数据，那么其host或者source字段无法在解析时识别，因此无法在数据流图中识别其后继数据处理对象


## 想法
结合这几种数据流构造和数据处理
vector可以静态编译配置文件，生成完整的数据流图，但是由于配置规则限制，导致数据流不够灵活。对于多种数据，同一输入，无法进行数据拆解，分别处理(或者说很难)

而logstash和splunk处理类似，logstash处理是将数据处理规则编码为包含条件分支和包含在条件分支下的语句块。因此虽然可以处理同一输入，多种数据的情况，但是数据的处理其实是混杂在同一块进行的，当数据处理过程容易产生冲突，并因此产生脏数据，造成不可预期的后果，而且管理不方便

在splunk中输入和输出的关系类似于vector，在数据输入时可以之间决定输出。因此这里也存在灵活性问题，事件的输出无法动态决定(例如可能会借由某个字段指定存储的索引等)。

splunk的输入和数据处理对象的关联，是通过特定字段值匹配来判断关联的，行为类似于logstash，多种数据间的处理和流向稍微清晰一些，但是仍然存在和logstash相同的问题。即数据处理对象可能存在冲突，即使可以生成数据流图加以分析，但是仍然不够清晰。例如经过某个数据处理对象后某个字段值匹配其他的数据处理对象，这个行为在数据进入时才能进行，静态编译识别无法将该行为画出，除非提供样例数据。

然而，即使提供样例数据，也仅能对一种数据展示数据流图，如果存在同一输入多种数据，那么如果样例数据不能包含各种情况，仍然无法描绘出完整的数据流图。而在复杂的数据环境中，没有人能够确保样例数据一定包含了各种场景。并且在抉择数据处理对象也会花费一定的计算资源。

结合以上的数据流，有一种初步想法来解决，不能静态生成数据流图同时支持动态抉择数据数量对象的问题。
在vector中通过`input`绑定输入，个人认为可以反过来通过`outputs`数据的输出。`ouputs`支持定义多个后继处理对象，并且支持某种条件匹配规则。
那么在动态选择数据处理对象时，可以通过`ouputs`配置的对应条件从多个后继数据处理对象中选择。而在静态编译配置时，可以通过`output`配置识别有限的后继数据处理对象。那么即使随着数据数据处理对象的增加，从整体看来，其后继有限，即使可能存在数据处理规则冲突，但是可以减少冲突的可能性。同时冲突时可以通过数据流图进行分析，因为描述的数据流图必然包含运行时所有可能运行的路径。
并且可以借鉴logstash在数据处理中新增加的pipeline概念，我们的配置中也可以增加多个pipeline，将相近的数据处理对象作为一个pipeline，那么可以进一步降低数据处理对象冲突可能性，管理上划分更清晰，reload更快速。



