## 数据完整一致
1. 数据包连续性问题
如何确保接收到的数据包是连续的，如果没有额外的校验阶段，那么接收到的数据包无法保证连续性。完成校验后还需要重发机制，这可能需要定制化的采集端，接收端，采用特定的协议。但是如果已经有了接收协议，那在接收端就能够完成数据包的拼接，不需要在数据处理阶段进行聚合。
如果不能保证数据包的连续性，就无法搞定第1个和第二个问题中提到的多个数据包的合并聚合问题。只能实现一个数据包拆分为多个事件

## 多行合并
1. charset问题
对接收数据进行字符集编码，需要保证进行的编码的数据块为正确长度的数据。否则会导致数据字符编码失败
可能会出现的场景有：
* 文件读取时按固定大小的二进制读取，从而导致数据长度失效。
    * 例如数据发送端每次发送64K，实际有效可进行字符集长度为63.99K，剩下的0.001K数据无法正确进行字符集编码，从而引起字符集编码失败
    * 目前发现的文件读取实现方式按字节读取，遇到"\n"时生成字节数据块后，再对数据块进行字符集编码
        * 但是带来的问题是，如果文件中的内容为1行长文本，可能会导致读取缓存区溢出而导致报错。

其他解决方案，按块读取，对块数据的进行字符编码，直到出现字符集出现编码错误位置，将错误位置到数据块尾部，作为下次字符集编码数据块的头部.
    * 例如python的codec包，在对文件进行读取，字符集编码时能够做到字符集编码在失败是指定失败位置，甚至提供接口，保存状态，将多个输入的可编码部分进行编码，下次字符集编码从失败位置开始
    * 但是如果没有提供能够在对数据块字符集编码失败时返回失败位置的接口，如果需要该实现需要各种实现字符集编码接口

2. 多行合并问题
对多个数据块进行多行合并时，单行数据不同的数据块间合并问题。
考虑如下文本与两个数据块
原始文本
```
line1
line2
line3
line4
```
数据块1
```
line1
line2
li
```
数据块2
```
ne3
line4
```
那么在splunk，logstash，vector中的多行合并处理模块中会将数据块进行多行合并处理为如下行
```
event0:
line1
line2
li

event1:
ne3
line4
```
这是因为在各个数据流中处理的数据单位为数据包，对应上面的两个数据块。
目前的实现中遵循了相同的思路，有以下原因：
* 数据复制带来的额外开销。如果input阶段(数据接收)时已经进行了一次数据复制，需要将`li`和`ne3`连接起来，就需要在数据传入多行合并的聚合阶段时进行一次完全的数据复制，而如果input阶段或发送方对数据块进行合理的拼接所需要进行的数据复制和移动更少。
    * 支持`li`和`ne3`的连接，需要进行一次数据复制，是因为正则库进行正则匹配需要传入一个字节数组或字符串，因此需要将保存在两个数组里的`li`和`line3\nline4`进行合并，这个过程中会造成一个数据块的完全复制，已经一个数据块的部分复制。
    * 可能的解决方法是通过hypscan正则库(或其他支持流正则匹配的库或自己改写)，该库支持stream模式的正则，通过该正则库的接口，可以在两个数据块上进行访问控制，并调用hypscan接口，完成正则匹配而不需要进行数组/字符串拼接
    * 其他可能的解决办法，通过某种方式实现数据一个结构体，满足一个数组行为，但底层是一个循环数组，从而免除数据移动行为。那么在正则库使用传入的数组时，以为是[]byte类型，但是实际是一个循环数组。
* 另一个原因是在某种情况下，无法将不同数据块数据进行拼接，例如网络传输的情况下，如何接收的两个数据包一定是连续的。
* 如果需要对多个数据包(数据块)进行拼接，除了需要保证数据包的连续性，还要需要在聚合处理对象处保存对应数据包，每个数据包需要一个唯一来源表示，而这个很难。
    * 例如文件类型的INPUT可以使用"主机名+文件路径"的方式唯一标识，但是仍然无法处理被rotate的文件，例如一个文件"a.log"被rotate为"a1.log"和"a2.log"，如果两个文件间数据块存在关联，那么聚合对象仍然无法将两种来源识别为一个唯一标识，需要INPUT或者发送方的额外处理。如果是网络类型的INPUT会更加复杂
* 即使需解决以上问题，那么在完成多个数据包的合并后，会存在部分数据缓存，作为下一个事件的一部分，那么这里可能需要一个超时机制(或其他，logstash采用超时)，用于将残余的数据生成事件，否则这些缓存数据可能会永远残留在多行合并对象中(如果对应来源没有后续数据)。

出现这种将单行切分到两个不同的数据块情况，主要在对文件的rotate处理粗犷的情况导致单行数据划分到不同的文件中。并且在这种情况下在多行合并的聚合阶段也可能无法将两个文件中的相同行进行合并，因为对数据流而言，这两个数据块的source不同，同样需要发送方进行数据块的处理并指明相同的source


## 时间提取方式抉择
基于正则还是基于strptime。
splunk中如果填写了`TIME_FORMAT`选项将会导致splunk使用strptime来对时间进行提取。`DATE_CONFIG`配置的时间处理器将不会生效，并且基于时间处理器的数据处理根据配置仅仅是能够从正则表达式中提取出各种字段，还需要针对字段进行额外的类型转换处理。

## 数据处理阶段事件流转方式抉择(数据传递)
两种方法:
1. 将各个数据处理对象通过管道或队列或者其他连接，event流转通过这些连接传给下一个数据处理对象。每个数据处理对象各自以线程形式启动
2. 各个数据处理对象都在同一线程内，主线程通过event数据选择下一个数据处理对象，直到结束。
第一种方法的好处在并行处理简单，不需要考虑event的顺序性，因为数据处理对象间
的连接对象已经保证了event的顺序性。缺点在于连接对象的复杂性以及带来的额外开销，例如event进入连接对象的开销，以及多线程环境下的线程安全问题等
第二种方法的好处在于，单线程比较简单，并且不需要连接对象(或者说连接对象仅仅是逻辑上的判断)，问题在于如果通过增加线程的方式增加吞吐率，那么会影响数据的顺序性，需要额外的机制保证连续数据负载到相同的处理线程。

## event数据结构
在vector中支持了两种格式的event，一种为metric，另一种为log。在vector官方的推荐中对监控指标这种数据采用metric作为event可以有效的提高性能。
LogEvent，需要能够包含各种数据类型，并动态支持添加新字段。在数据处理过程中，访问已有字段是一种常见行为，这要求LogEvent查找已有字段需要有良好的性能，合理选择LogEvent数据结构，有利于提高数据处理性能。
vector中LogEvent数据结构如下，这里有个问题，那就是数据流处理中需要使用的字段，这些字段不需要额外输出，但是在整个数据流中不可缺少(例如某个transform生成字段为后续transform服务，但不需要输出)，那么vector就需要额外删除字段。在logstash中这种类型的字段又被称为@metadata字段
```rust
pub struct LogEvent {
    fields: BTreeMap<String, Value>,
}
```

MetricEvent在数据结构上比较复杂，这是开发者对已有监控类型数据的认知，提前在这种数据结构上实现了一些基于监控数据的优化操作

## 聚合操作
如果能够解决数据包接收的连续性问题，例如INPUT对应的是一个mysql数据库或我们的csearch。
那么在有些需求，会需要对数据包进行聚合操作，例如对某个字段值进行分组。这种聚合行为是否应该发生在数据流上?

